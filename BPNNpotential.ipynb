{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d369744",
   "metadata": {},
   "source": [
    "**J. Behler and M. Parrinello, Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces, PRL 98, 146401 (2007)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318484ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:03:28.735611Z",
     "start_time": "2022-09-27T16:03:20.881362Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import exp, pow, sqrt, tanh, pi, cos, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Sequence, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "#from sklearn.model_selection import train_test_split\n",
    "torch.set_printoptions(threshold=np.inf)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4bb53",
   "metadata": {},
   "source": [
    "![TOC.jpg](./TOC.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8da734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:03:28.794939Z",
     "start_time": "2022-09-27T16:03:28.746949Z"
    }
   },
   "outputs": [],
   "source": [
    "#Require: Num_atom, Num_config, Num_layer, and Num_node\n",
    "Num_atom = 14  # QM area\n",
    "Num_atom_all = 6041 # QM and MM area\n",
    "Num_config = 1000 #Number of snapshots\n",
    "Num_feature = 3\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "Num_train = int(Num_config * train_ratio) # Number of training set\n",
    "Num_valid = int(Num_config * validation_ratio) # Number of valid set\n",
    "Num_test = Num_config -  Num_train - Num_valid #Number of test set\n",
    "\n",
    "element_list = ['C', 'C', 'H', 'H', 'H', 'C', 'H', 'H', 'O', 'C', 'C', 'H', 'H', 'H']\n",
    "unique_element_list = np.unique(element_list)\n",
    "Num_element = unique_element_list.size\n",
    "\n",
    "element_index_in_molecule = []\n",
    "for i_element in range(Num_element):\n",
    "    new_list = [i for i in range(np.array(element_list).size) if element_list[i] == unique_element_list[i_element]]\n",
    "    element_index_in_molecule.append(new_list)\n",
    "    \n",
    "type_list = []\n",
    "for i_type in range(np.array(element_list).size):\n",
    "    j, = np.where(unique_element_list == element_list[i_type])\n",
    "    type_list.append(j[0])\n",
    "    \n",
    "C_atom = element_index_in_molecule[0] #index of C\n",
    "O_atom = element_index_in_molecule[1]\n",
    "H_atom = element_index_in_molecule[2]\n",
    "\n",
    "charge_of_water = [-0.834, 0.417, 0.417] # 0 and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab165451",
   "metadata": {},
   "outputs": [],
   "source": [
    "## symmetric_haparm list\n",
    "\n",
    "def random_hparam(Num_hparams,Num_atomtype,Range_hparam=[[0.1,1.0],[0.0,4.0],[0.1,1.2],[6,8]]):\n",
    "    symmetric_hparam=[]\n",
    "    Num_hparam=len(Range_hparam)\n",
    "    Delta_hpara=np.zeros(Num_hparam)\n",
    "    for num_hpara_i in range(Num_hparam):\n",
    "        Delta_hpara[num_hpara_i] = Range_hparam[num_hpara_i][1]-Range_hparam[num_hpara_i][0]\n",
    "    #print(Delta_hpara)\n",
    "        \n",
    "    for Hparams_i in range(Num_hparams):\n",
    "        tem_np=np.zeros((Num_atomtype,Num_hparam))\n",
    "        for Atomtype_j in range(Num_atomtype):\n",
    "            for num_hpara_k in range(Num_hparam):\n",
    "                tem_np[Atomtype_j,num_hpara_k]=random.random()*Delta_hpara[num_hpara_k]+Range_hparam[num_hpara_k][0]\n",
    "        symmetric_hparam.append(unitconversion(tem_np))\n",
    "        #symmetric_hparam.append(tem_np)\n",
    "    symmetric_hparam.append('stationary')\n",
    "    return symmetric_hparam\n",
    "\n",
    "def unitconversion(list,wcba=(1/0.52917724)**2):\n",
    "    result = [  [i*wcba  for i in j] for j in list] # bohr to A\n",
    "    return result\n",
    "\n",
    "'''\n",
    "symmetric_hparam=[ [0.4, 0.0, 0.2, 6.0],\n",
    "                   [0.1, 0.0, 0.4, 6.0],\n",
    "                   [0.4, 0.0, 0.8, 6.0] ]   #geta, g1rs, g2zeta, rc\n",
    "                   '''\n",
    "cba= 0.52917724\n",
    "wcba = (1/cba)**2\n",
    "\n",
    "# random get hparam\n",
    "symmetric_hparam=random_hparam(Num_hparams=100,Num_atomtype=Num_element,Range_hparam=[[0.1,1.0],[0.0/wcba,4.0/wcba],[0.1,1.2],[6/wcba,6/wcba]])\n",
    "\n",
    "Num_feature = 2*(len(symmetric_hparam)-1)+1\n",
    "\n",
    "#joint hparam\n",
    "symmetric_hparam1=[ [0.4, 0.0, 0.2, 6.0],\n",
    "                   [0.1, 0.0, 0.8, 6.0],\n",
    "                   [0.4, 0.0, 0.4, 6.0] ]   #geta, g1rs, g2zeta, rc\n",
    "symmetric_hparam2=[ [0.6, 0.0, 0.3, 6.0],\n",
    "                   [0.2, 0.0, 1.2, 6.0],\n",
    "                   [0.6, 0.0, 0.6, 6.0] ]   #geta, g1rs, g2zeta, rc\n",
    "symmetric_hparam3=[ [0.8, 0.0, 0.4, 6.0],\n",
    "                   [0.2, 0.0, 1.6, 6.0],\n",
    "                   [0.8, 0.0, 0.8, 6.0] ]   #geta, g1rs, g2zeta, rc\n",
    "def unitconversion(list,wcba=(1/0.52917724)**2):\n",
    "    result = [  [i*wcba  for i in j] for j in list] # bohr to A\n",
    "    return result\n",
    "\n",
    "symmetric_hparam=[unitconversion(symmetric_hparam1),unitconversion(symmetric_hparam2)\n",
    "                 ,unitconversion(symmetric_hparam3),'stationary']\n",
    "Num_feature = 2*(len(symmetric_hparam)-1)+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77560d92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:03:28.809934Z",
     "start_time": "2022-09-27T16:03:28.794939Z"
    }
   },
   "outputs": [],
   "source": [
    "#read coordinate and energy of each snapshot\n",
    "def readCxyz(n_files, path, n_atoms):\n",
    "    '''Read Cartesian coordinates'''\n",
    "    coord = np.empty([n_files, n_atoms, 3])\n",
    "    for idx_file in range( n_files ):\n",
    "        filename = path + str(idx_file+1) + '.xyz'\n",
    "        fc = open(filename)\n",
    "#        line = fc.readline()\n",
    "#        line = fc.readline()\n",
    "        for idx_atom in range(n_atoms):\n",
    "            line = fc.readline()\n",
    "            if not line:\n",
    "               break\n",
    "            _, coord_x, coord_y, coord_z = line.strip().split()\n",
    "            coord[idx_file][idx_atom][0] = float(coord_x)\n",
    "            coord[idx_file][idx_atom][1] = float(coord_y)\n",
    "            coord[idx_file][idx_atom][2] = float(coord_z)\n",
    "        fc.close()\n",
    "    return coord\n",
    "\n",
    "\n",
    "def readE(n_data, filename):\n",
    "    ''' Read E '''\n",
    "    energy = np.empty(n_data)\n",
    "    e1 = open(filename,'r')\n",
    "    for idx_data in range( n_data ):\n",
    "        line = e1.readline()\n",
    "        if not line:\n",
    "           break\n",
    "        energy[idx_data] = float(line.strip().split()[1])\n",
    "    e1.close()\n",
    "    return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf0b9b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:03:53.077927Z",
     "start_time": "2022-09-27T16:03:28.809934Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_E = 'Energy.dat'\n",
    "path_xyz='./xyz/'\n",
    "coord = readCxyz(Num_config, path_xyz, Num_atom_all)\n",
    "energy = readE(Num_config, filename_E)\n",
    "\n",
    "coord_train, coord_test, energy_train, energy_test = train_test_split(coord, energy, test_size=1 - train_ratio)\n",
    "coord_valid, coord_test, energy_valid, energy_test = train_test_split(coord_test, energy_test, test_size=test_ratio/(test_ratio + validation_ratio)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889638cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:03:53.107968Z",
     "start_time": "2022-09-27T16:03:53.077927Z"
    }
   },
   "outputs": [],
   "source": [
    "#Package parameters to class\n",
    "class nn_param (object):\n",
    "    def __init__(self):\n",
    "        self.geta = None\n",
    "        self.g1rs = None\n",
    "        self.g2zeta = None\n",
    "        self.ctrc = None\n",
    "        self.Num_atom = None\n",
    "        self.Num_atom_all = None\n",
    "        self.Num_conf = None\n",
    "        self.lenhparam = None\n",
    "\n",
    "    def configuration(self, Num_atom, Num_atom_all, Num_conf, type_list, symmetric_hparam):\n",
    "        '''specify'''\n",
    "        self.Num_atom = Num_atom\n",
    "        self.Num_atom_all = Num_atom_all\n",
    "        self.Num_conf = Num_conf\n",
    "        \n",
    "        \n",
    "        if symmetric_hparam[-1] =='fixed' or 'stationary':\n",
    "            self.lenhparam=len(symmetric_hparam)-1\n",
    "            self.geta   = np.zeros((self.Num_atom,self.lenhparam))\n",
    "            self.g1rs  = np.zeros((self.Num_atom,self.lenhparam))\n",
    "            self.g2zeta   = np.zeros((self.Num_atom,self.lenhparam))\n",
    "            self.ctrc  = np.zeros((self.Num_atom,self.lenhparam))\n",
    "        else:\n",
    "            print('ERROR:symmetric_hparam[-1] may be str(fixed) ')\n",
    "        \n",
    "\n",
    "        for i in range(self.Num_atom):\n",
    "            for j in range(self.lenhparam):\n",
    "                self.geta[i,j]  = symmetric_hparam[j][type_list[i]][0]\n",
    "                self.g1rs[i,j] = symmetric_hparam[j][type_list[i]][1]\n",
    "                self.g2zeta[i,j]  = symmetric_hparam[j][type_list[i]][2]\n",
    "                self.ctrc[i,j] = symmetric_hparam[j][type_list[i]][3]\n",
    "\n",
    "param_train = nn_param() #all the input parameters in a class\n",
    "param_valid = nn_param()\n",
    "param_test  = nn_param()\n",
    "\n",
    "param_train.configuration(Num_atom, Num_atom_all, Num_train, type_list, symmetric_hparam)\n",
    "param_valid.configuration(Num_atom, Num_atom_all, Num_valid, type_list, symmetric_hparam)\n",
    "param_test.configuration(Num_atom, Num_atom_all, Num_test, type_list, symmetric_hparam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d79ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:03:53.167687Z",
     "start_time": "2022-09-27T16:03:53.117612Z"
    }
   },
   "outputs": [],
   "source": [
    "class molecule(object):\n",
    "    def __init__(self, nn_param):\n",
    "        self.Num_atom = nn_param.Num_atom\n",
    "        self.Num_atom_all = nn_param.Num_atom_all\n",
    "        self.geta = nn_param.geta\n",
    "        self.g1rs = nn_param.g1rs\n",
    "        self.g2zeta = nn_param.g2zeta\n",
    "        self.ctrc = nn_param.ctrc\n",
    "        self.Len_hparam =nn_param.lenhparam\n",
    "        \n",
    "        self.g1 = np.zeros((self.Num_atom,self.Len_hparam))\n",
    "        self.g2 = np.zeros((self.Num_atom,self.Len_hparam))\n",
    "        self.gv = np.zeros((self.Num_atom,1))\n",
    "    \n",
    "    def distance(self, r1, r2):\n",
    "\n",
    "        return sqrt((r1[0]-r2[0])**2 + (r1[1]-r2[1])**2 + (r1[2]-r2[2])**2)\n",
    "\n",
    "    def cosijk(self, d1, d2, d3):\n",
    "\n",
    "        return (d1**2 + d3**2 - d2**2)/(2 * d1 * d3)\n",
    "        \n",
    "    def calc_feature(self, coord, charge_of_water):\n",
    "        \n",
    "        #dist\n",
    "        dist = np.zeros([self.Num_atom, self.Num_atom_all])\n",
    "        for i_atom in range(self.Num_atom):\n",
    "            for j_atom in range(self.Num_atom_all):\n",
    "                if j_atom == i_atom:\n",
    "                    pass\n",
    "                else:\n",
    "                    dist[i_atom, j_atom] = self.distance(coord[i_atom, :], coord[j_atom, :])\n",
    "        #fc\n",
    "        fc = np.zeros([self.Num_atom, self.Num_atom,self.Len_hparam])          \n",
    "        for n_lenhparam in range(self.Len_hparam):\n",
    "            for i_atom in range(self.Num_atom):\n",
    "                for j_atom in range(i_atom+1, self.Num_atom):\n",
    "                    dist_ij = dist[i_atom, j_atom]\n",
    "                    if dist_ij <= self.ctrc[i_atom,n_lenhparam]:\n",
    "                        fc[i_atom, j_atom,n_lenhparam] =  0.5 * ( cos( pi * dist_ij / self.ctrc[i_atom,n_lenhparam] ) + 1 )\n",
    "                        #fc_tanh[i_atom, j_atom,n_lenhparam]=tanh(1-(dist_ij / rc[i_atom,n_lenhparam])) **3\n",
    "                    else:\n",
    "                        fc[i_atom, j_atom,n_lenhparam] =  0.0\n",
    "                    fc[j_atom, i_atom,n_lenhparam] = fc[i_atom, j_atom,n_lenhparam]\n",
    "                    #fc_tanh[j_atom, i_atom,n_lenhparam] = fc_tanh[i_atom, j_atom,n_lenhparam]\n",
    "        #cal g1\n",
    "        for n_lenhparam in range(self.Len_hparam):\n",
    "            for i_atom in range(self.Num_atom):\n",
    "                for j_atom in range(self.Num_atom):\n",
    "                    if j_atom == i_atom:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.g1[i_atom,n_lenhparam] += exp( -self.geta[i_atom,n_lenhparam] * \\\n",
    "                        ( (dist[i_atom, j_atom] - self.g1rs[i_atom,n_lenhparam])**2 ) )* fc[j_atom, i_atom,n_lenhparam]\n",
    "\n",
    "                        \n",
    "                        \n",
    "        #cal g2\n",
    "        for n_lenhparam in range(self.Len_hparam):\n",
    "            for i_atom in range(self.Num_atom):\n",
    "                for j_atom in range(self.Num_atom):\n",
    "                    if j_atom == i_atom:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for k_atom in range(self.Num_atom):\n",
    "                            if k_atom  == j_atom :\n",
    "                                pass\n",
    "                            elif k_atom == i_atom:\n",
    "                                pass\n",
    "                            else:\n",
    "                                self.g2[i_atom,n_lenhparam] += 2**(1-self.g2zeta[i_atom,n_lenhparam])*( ( 1 + self.cosijk( dist[j_atom, k_atom], dist[i_atom, k_atom], dist[i_atom, j_atom] ) )\\\n",
    "                                **(self.g2zeta[i_atom,n_lenhparam])) *exp( -self.geta[i_atom,n_lenhparam] * ( dist[i_atom, j_atom]**2 + dist[i_atom, k_atom]**2 + dist[i_atom, k_atom]**2) ) \\\n",
    "                                *fc[j_atom, i_atom,n_lenhparam] * fc[k_atom, i_atom,n_lenhparam] * fc[j_atom, k_atom,n_lenhparam]\n",
    "            \n",
    "        #cal gv\n",
    "        for i_atom in range(self.Num_atom):\n",
    "            for j_atom in range(self.Num_atom, self.Num_atom_all):\n",
    "                if (j_atom - Num_atom) % 3 == 0:\n",
    "                    self.gv[i_atom,0] += charge_of_water[0]/(dist[i_atom, j_atom]**2)\n",
    "                else:\n",
    "                    self.gv[i_atom,0] += charge_of_water[1]/(dist[i_atom, j_atom]**2)                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "150cfcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:12:15.800381Z",
     "start_time": "2022-09-27T16:03:53.167687Z"
    }
   },
   "outputs": [],
   "source": [
    "snapshots_train = [molecule(param_train) for i_conf in range(Num_train)]\n",
    "snapshots_valid = [molecule(param_valid) for i_conf in range(Num_valid)]\n",
    "snapshots_test = [molecule(param_test) for i_conf in range(Num_test)]\n",
    "\n",
    "for i_conf in range(Num_train):\n",
    "    snapshots_train[i_conf].calc_feature(coord_train[i_conf,:,:], charge_of_water)\n",
    "    \n",
    "for i_conf in range(Num_valid):\n",
    "    snapshots_valid[i_conf].calc_feature(coord_valid[i_conf,:,:], charge_of_water)\n",
    "    \n",
    "for i_conf in range(Num_test):\n",
    "    snapshots_test[i_conf].calc_feature(coord_test[i_conf,:,:], charge_of_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fd5306e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:12:15.847258Z",
     "start_time": "2022-09-27T16:12:15.805370Z"
    }
   },
   "outputs": [],
   "source": [
    "input_feature_train = np.empty(shape=[0, Num_atom * Num_feature])\n",
    "input_feature_valid = np.empty(shape=[0, Num_atom * Num_feature])\n",
    "input_feature_test  = np.empty(shape=[0, Num_atom * Num_feature])\n",
    "\n",
    "for i_conf in range(Num_train):\n",
    "    input_feature_train = np.append(input_feature_train, [np.concatenate((snapshots_train[i_conf].g1.ravel(), snapshots_train[i_conf].g2.ravel(), snapshots_train[i_conf].gv.ravel()))], axis = 0)\n",
    "    \n",
    "for i_conf in range(Num_valid):\n",
    "    input_feature_valid = np.append(input_feature_valid, [np.concatenate((snapshots_valid[i_conf].g1.ravel(), snapshots_valid[i_conf].g2.ravel(), snapshots_valid[i_conf].gv.ravel()))], axis = 0)\n",
    "    \n",
    "for i_conf in range(Num_test):\n",
    "    input_feature_test = np.append(input_feature_test, [np.concatenate((snapshots_test[i_conf].g1.ravel(), snapshots_test[i_conf].g2.ravel(), snapshots_test[i_conf].gv.ravel()))], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70caf97d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:12:16.026092Z",
     "start_time": "2022-09-27T16:12:15.850251Z"
    }
   },
   "outputs": [],
   "source": [
    "#pytorch tensordataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_train = torch.from_numpy(input_feature_train.astype(np.float32)).to(device)\n",
    "input_valid = torch.from_numpy(input_feature_valid.astype(np.float32)).to(device)\n",
    "input_test = torch.from_numpy(input_feature_test.astype(np.float32)).to(device)\n",
    "\n",
    "output_train = torch.from_numpy(energy_train.astype(np.float32)).to(device)\n",
    "output_valid = torch.from_numpy(energy_valid.astype(np.float32)).to(device)\n",
    "output_test = torch.from_numpy(energy_test.astype(np.float32)).to(device)\n",
    "\n",
    "\n",
    "DLparams = {'batch_size': 40,\n",
    "            'shuffle': True}\n",
    "\n",
    "val_DLparams = {'batch_size': Num_valid,\n",
    "                'shuffle': False}\n",
    "\n",
    "test_DLparams = {'batch_size': Num_test,\n",
    "                'shuffle': False}\n",
    "\n",
    "dataset_train = TensorDataset(input_train, output_train)\n",
    "dataset_valid = TensorDataset(input_valid, output_valid)\n",
    "dataset_test = TensorDataset(input_test, output_test)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, **DLparams)\n",
    "loader_valid = DataLoader(dataset_valid, **val_DLparams)\n",
    "loader_test = DataLoader(dataset_test, **test_DLparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68392858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:12:16.058007Z",
     "start_time": "2022-09-27T16:12:16.032078Z"
    }
   },
   "outputs": [],
   "source": [
    "#training process\n",
    "class PlNeuralNet(pl.LightningModule):\n",
    "    def __init__(self, n_net=1, n1=10, lr=1.e-2,Num_feature=3):\n",
    "        super().__init__()\n",
    "        subnet = nn.Sequential(\n",
    "            nn.Linear(Num_feature, n1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n1, n1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n1, 1)\n",
    "        )\n",
    "        subnets = [subnet for i in range(n_net)] \n",
    "        self.model = nn.ModuleList(subnets)\n",
    "        self.learning_rate = lr\n",
    "        self.Num_feature=Num_feature\n",
    "\n",
    "    def forward(self, x):    \n",
    "        b_size, n_atom = x.shape\n",
    "        atomic_E = torch.zeros((b_size, n_atom//Num_feature)).to(device)\n",
    "        for i_atom in range(Num_atom):\n",
    "            i_subnet = type_list[i_atom]\n",
    "            atomic_E[:,i_atom] = torch.flatten(self.model[i_subnet](x[:,self.Num_feature*i_atom:self.Num_feature*(i_atom+1)]))\n",
    "        Eout  = torch.sum(atomic_E, axis = 1)\n",
    "        return Eout\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xy, z = batch\n",
    "        z_pred = self(xy)\n",
    "        lossfn = nn.MSELoss()\n",
    "        loss = lossfn(z_pred.squeeze(), z)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        xy, z = batch\n",
    "        z_pred = self(xy)\n",
    "        lossfn = nn.MSELoss()\n",
    "        lossval = lossfn(z_pred.squeeze(), z)\n",
    "        self.log('valloss', lossval)\n",
    "        return lossval\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        xy, z = batch\n",
    "        z_pred = self(xy)\n",
    "        lossfn = nn.MSELoss()\n",
    "        losstest = lossfn(z_pred.squeeze(), z)\n",
    "        self.log('testloss', losstest)\n",
    "        return losstest\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "#        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "#        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1.E-5)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.97),\n",
    "                     'interval': 'epoch',\n",
    "                     'frequency': 1,\n",
    "                    }\n",
    "        return [optimizer] #, [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec4c991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:22:54.590726Z",
     "start_time": "2022-09-27T16:12:16.061000Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs_csv/lightning_logs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | ModuleList | 201   \n",
      "-------------------------------------\n",
      "201       Trainable params\n",
      "0         Non-trainable params\n",
      "201       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\wjn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514810d0ea8345c3b7e17d0ff10d817c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m PlNeuralNet(n_net, n1, learning_rate,Num_feature)\n\u001b[0;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, loader_train, loader_valid)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mvalidate(dataloaders \u001b[38;5;241m=\u001b[39m loader_valid, ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:862\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;124;03mPerform one evaluation epoch over the test set.\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03mIt's separated from fit to make sure you never run on your test set until you want to.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m    The length of the list corresponds to the number of test dataloaders used.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[1;32m--> 862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:902\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# links data to the trainer\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(model, test_dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[1;32m--> 902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__set_ckpt_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_provided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_connected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    904\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tested_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path  \u001b[38;5;66;03m# TODO: remove in v1.8\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;66;03m# run test\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1413\u001b[0m, in \u001b[0;36mTrainer.__set_ckpt_path\u001b[1;34m(self, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_dev_run:\n\u001b[0;32m   1409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m   1410\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot execute `.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(ckpt_path=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)` with `fast_dev_run=True`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1411\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pass an exact checkpoint path to `.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(ckpt_path=...)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1412\u001b[0m         )\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m   1414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(ckpt_path=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)` is set but `ModelCheckpoint` is not configured to save the best model.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1415\u001b[0m     )\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;66;03m# load best weights\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_callback, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint   \n",
    "num_epochs = 1000\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k = -1, save_last = False, every_n_epochs = 100)#100 epochs to save once \n",
    "csv_logger = pl_loggers.CSVLogger('logs_csv/', flush_logs_every_n_steps = 15)\n",
    "trainer = pl.Trainer(max_epochs = num_epochs, log_every_n_steps = 15, logger = csv_logger, callbacks = [checkpoint_callback], accelerator = \"auto\", devices = \"auto\")\n",
    "\n",
    "#trainer = pl.Trainer(max_epochs=num_epochs, logger=csv_logger, gpus=1)\n",
    "n1 = 10\n",
    "n_net = Num_element\n",
    "learning_rate = 1.e-2\n",
    "model = PlNeuralNet(n_net, n1, learning_rate,Num_feature)\n",
    "\n",
    "trainer.fit(model, loader_train, loader_valid)\n",
    "trainer.test(dataloaders = loader_test, ckpt_path = 'best')\n",
    "trainer.validate(dataloaders = loader_valid, ckpt_path = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameters in model.named_parameters():\n",
    "        print(name, ':', parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82644306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:36:43.136584Z",
     "start_time": "2022-09-27T16:36:42.625692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Errors')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot \n",
    "import pandas as pd\n",
    "\n",
    "loss_pack = pd.read_csv(\"logs_csv/lightning_logs/version_0/metrics.csv\")\n",
    "loss_valid = loss_pack.valloss.dropna()\n",
    "loss_train = loss_pack.train_loss.dropna()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(loss_train,'--',loss_valid,'.')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Errors\")\n",
    "# Show the predicted surface, and compare to the reference one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23b2ca71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:22:57.921968Z",
     "start_time": "2022-09-27T16:22:57.891502Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_net = 1, n1 = 10,Num_feature=3):  #n1 is the number of neurons for the first layer\n",
    "        super().__init__()\n",
    "        subnet = nn.Sequential(\n",
    "            nn.Linear(Num_feature, n1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n1, n1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n1, 1)\n",
    "        )\n",
    "        subnets = [subnet for i in range(n_net)] \n",
    "        self.model = nn.ModuleList(subnets)\n",
    "        self.Num_feature=Num_feature\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        b_size, n_atom = x.shape\n",
    "        atomic_E = torch.zeros((b_size, n_atom//Num_feature)).to(device)\n",
    "        for i_atom in range(Num_atom):\n",
    "            i_subnet = type_list[i_atom]\n",
    "            atomic_E[:,i_atom] = torch.flatten(self.model[i_subnet](x[:,self.Num_feature*i_atom:self.Num_feature*(i_atom+1)]))\n",
    "        Eout  =torch.sum(atomic_E, axis = 1)\n",
    "        return Eout\n",
    "\n",
    "    \n",
    "\n",
    "def train_loop(dataloader, model, lossfn, optimizer, i_epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (input_feature, target) in enumerate(dataloader):\n",
    "        pred = model(input_feature)   # call model.forward()\n",
    "        loss = lossfn(pred.squeeze(), target)\n",
    "    \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch+1 == ceil(len(dataloader.dataset)/DLparams['batch_size']) and i_epoch % 20 == 0:\n",
    "            loss, current = loss.item(), batch * len(input_feature)\n",
    "            print(f\"epoch: {i_epoch:>4d} loss: {loss:>7.3f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67920b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T16:34:03.824616Z",
     "start_time": "2022-09-27T16:22:57.924923Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0 loss:   9.402  [  540/  750]\n",
      "epoch:   20 loss:   5.300  [  540/  750]\n",
      "epoch:   40 loss:   3.970  [  540/  750]\n",
      "epoch:   60 loss:   3.442  [  540/  750]\n",
      "epoch:   80 loss:   5.071  [  540/  750]\n",
      "epoch:  100 loss:   3.487  [  540/  750]\n",
      "epoch:  120 loss:   2.863  [  540/  750]\n",
      "epoch:  140 loss:   3.236  [  540/  750]\n",
      "epoch:  160 loss:   5.052  [  540/  750]\n",
      "epoch:  180 loss:   4.325  [  540/  750]\n",
      "epoch:  200 loss:   3.048  [  540/  750]\n",
      "epoch:  220 loss:   4.131  [  540/  750]\n",
      "epoch:  240 loss:   4.309  [  540/  750]\n",
      "epoch:  260 loss:   4.186  [  540/  750]\n",
      "epoch:  280 loss:   3.562  [  540/  750]\n",
      "epoch:  300 loss:   2.121  [  540/  750]\n",
      "epoch:  320 loss:   3.499  [  540/  750]\n",
      "epoch:  340 loss:   3.817  [  540/  750]\n",
      "epoch:  360 loss:   3.894  [  540/  750]\n",
      "epoch:  380 loss:   3.934  [  540/  750]\n",
      "epoch:  400 loss:   3.644  [  540/  750]\n",
      "epoch:  420 loss:   2.109  [  540/  750]\n",
      "epoch:  440 loss:   3.041  [  540/  750]\n",
      "epoch:  460 loss:   6.281  [  540/  750]\n",
      "epoch:  480 loss:   3.493  [  540/  750]\n",
      "epoch:  500 loss:   3.447  [  540/  750]\n",
      "epoch:  520 loss:   3.062  [  540/  750]\n",
      "epoch:  540 loss:   3.834  [  540/  750]\n",
      "epoch:  560 loss:   5.862  [  540/  750]\n",
      "epoch:  580 loss:   2.613  [  540/  750]\n",
      "epoch:  600 loss:   3.222  [  540/  750]\n",
      "epoch:  620 loss:   3.484  [  540/  750]\n",
      "epoch:  640 loss:   3.690  [  540/  750]\n",
      "epoch:  660 loss:   3.529  [  540/  750]\n",
      "epoch:  680 loss:   2.537  [  540/  750]\n",
      "epoch:  700 loss:   4.147  [  540/  750]\n",
      "epoch:  720 loss:   4.066  [  540/  750]\n",
      "epoch:  740 loss:   4.640  [  540/  750]\n",
      "epoch:  760 loss:   4.504  [  540/  750]\n",
      "epoch:  780 loss:   3.832  [  540/  750]\n",
      "epoch:  800 loss:   4.626  [  540/  750]\n",
      "epoch:  820 loss:   4.078  [  540/  750]\n",
      "epoch:  840 loss:   3.301  [  540/  750]\n",
      "epoch:  860 loss:   4.024  [  540/  750]\n",
      "epoch:  880 loss:   2.403  [  540/  750]\n",
      "epoch:  900 loss:   2.692  [  540/  750]\n",
      "epoch:  920 loss:   4.732  [  540/  750]\n",
      "epoch:  940 loss:   2.810  [  540/  750]\n",
      "epoch:  960 loss:   4.679  [  540/  750]\n",
      "epoch:  980 loss:   3.310  [  540/  750]\n",
      "epoch: 1000 loss:   3.322  [  540/  750]\n",
      "epoch: 1020 loss:   4.305  [  540/  750]\n",
      "epoch: 1040 loss:   3.680  [  540/  750]\n",
      "epoch: 1060 loss:   3.989  [  540/  750]\n",
      "epoch: 1080 loss:   3.427  [  540/  750]\n",
      "epoch: 1100 loss:   5.411  [  540/  750]\n",
      "epoch: 1120 loss:   4.326  [  540/  750]\n",
      "epoch: 1140 loss:   4.081  [  540/  750]\n",
      "epoch: 1160 loss:   3.826  [  540/  750]\n",
      "epoch: 1180 loss:   4.735  [  540/  750]\n",
      "epoch: 1200 loss:   4.008  [  540/  750]\n",
      "epoch: 1220 loss:   4.602  [  540/  750]\n",
      "epoch: 1240 loss:   3.057  [  540/  750]\n",
      "epoch: 1260 loss:   4.257  [  540/  750]\n",
      "epoch: 1280 loss:   3.112  [  540/  750]\n",
      "epoch: 1300 loss:   4.688  [  540/  750]\n",
      "epoch: 1320 loss:   3.298  [  540/  750]\n",
      "epoch: 1340 loss:   3.204  [  540/  750]\n",
      "epoch: 1360 loss:   1.839  [  540/  750]\n",
      "epoch: 1380 loss:   4.731  [  540/  750]\n",
      "epoch: 1400 loss:   4.619  [  540/  750]\n",
      "epoch: 1420 loss:   3.352  [  540/  750]\n",
      "epoch: 1440 loss:   2.959  [  540/  750]\n",
      "epoch: 1460 loss:   6.064  [  540/  750]\n",
      "epoch: 1480 loss:   4.141  [  540/  750]\n",
      "epoch: 1500 loss:   3.930  [  540/  750]\n",
      "epoch: 1520 loss:   3.236  [  540/  750]\n",
      "epoch: 1540 loss:   2.525  [  540/  750]\n",
      "epoch: 1560 loss:   2.910  [  540/  750]\n",
      "epoch: 1580 loss:   2.803  [  540/  750]\n",
      "epoch: 1600 loss:   3.797  [  540/  750]\n",
      "epoch: 1620 loss:   3.208  [  540/  750]\n",
      "epoch: 1640 loss:   2.396  [  540/  750]\n",
      "epoch: 1660 loss:   4.716  [  540/  750]\n",
      "epoch: 1680 loss:   4.784  [  540/  750]\n",
      "epoch: 1700 loss:   5.637  [  540/  750]\n",
      "epoch: 1720 loss:   4.285  [  540/  750]\n",
      "epoch: 1740 loss:   1.811  [  540/  750]\n",
      "epoch: 1760 loss:   4.647  [  540/  750]\n",
      "epoch: 1780 loss:   6.487  [  540/  750]\n",
      "epoch: 1800 loss:   2.519  [  540/  750]\n",
      "epoch: 1820 loss:   3.832  [  540/  750]\n",
      "epoch: 1840 loss:   4.093  [  540/  750]\n",
      "epoch: 1860 loss:   2.923  [  540/  750]\n",
      "epoch: 1880 loss:   5.116  [  540/  750]\n",
      "epoch: 1900 loss:   5.624  [  540/  750]\n",
      "epoch: 1920 loss:   4.560  [  540/  750]\n",
      "epoch: 1940 loss:   3.370  [  540/  750]\n",
      "epoch: 1960 loss:   2.290  [  540/  750]\n",
      "epoch: 1980 loss:   5.785  [  540/  750]\n",
      "Done with Training!\n"
     ]
    }
   ],
   "source": [
    "# Training of Machine Learning Model\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 2000\n",
    "n_net = Num_element\n",
    "n1 = 10 \n",
    "model = NeuralNetwork(n_net, n1,Num_feature).to(device)\n",
    "lossfn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1.E-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.97)\n",
    "\n",
    "for i_epoch in range(num_epochs):\n",
    "    train_loop(loader_train, model, lossfn, optimizer, i_epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "print(\"Done with Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604baded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size1 (100, 98)\n",
      "begin kernel\n",
      "expect 0.009999999536667019\n",
      "end kernel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab72820ce5644c47a1ee5abc11102c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "#cal shap of each feature if need\n",
    "\n",
    "#model3 = NeuralNetwork().to(device)\n",
    "\n",
    "batch = next(iter(loader_test))\n",
    "inputx, _ = batch\n",
    "\n",
    "dataX=inputx.unsqueeze(1).numpy()\n",
    "dataX=inputx.numpy().copy()\n",
    "print('size1',np.shape(dataX))\n",
    "\n",
    "def batch_predict(data, model=model):\n",
    "\n",
    "    X_tensor = torch.from_numpy(data.copy()).float()\n",
    "    #X_tensor =data\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    #X_tensor = X_tensor.squeeze(dim=1).to(device)\n",
    "    X_tensor = X_tensor.to(device)\n",
    "    logits = model(X_tensor)\n",
    "    #print(logits)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=0)\n",
    "    #print('probs=',type(probs.detach().cpu().numpy()))\n",
    "    return probs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "print('begin kernel')\n",
    "explainer = shap.KernelExplainer(batch_predict,dataX)\n",
    "print('expect',explainer.expected_value)\n",
    "print('end kernel')\n",
    "\n",
    "\n",
    "shap_values = explainer.shap_values(dataX)\n",
    "print('value',shap_values,len(shap_values),len(shap_values.shape))\n",
    "print('end exp')\n",
    "\n",
    "\n",
    "#fig\n",
    "fig = plt.figure()\n",
    "\n",
    "shap.summary_plot(shap_values=shap_values,\n",
    "                 features=dataX,\n",
    "                 feature_names=[str(di) for di in range(len(dataX[0]))],\n",
    "                 #feature_names=['sepal length','sepal width','petal length','petal width'],\n",
    "                 plot_type='bar',max_display = 42,show = False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1647a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
